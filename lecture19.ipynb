{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel density estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel density estimation is the process of estimating an unknown probability density function using a kernel function $K(u)$\n",
    "\n",
    "In the following example, we will estimate the PDF of a bimodal distribution: a mixture of two normal distributions with locations at -1 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Location, scale and weight for the two distributions\n",
    "dist1_loc, dist1_scale, weight1 = -1 , .5, .25\n",
    "dist2_loc, dist2_scale, weight2 = 1 , .5, .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.distributions.mixture_rvs import mixture_rvs\n",
    "# Sample from a mixture of distributions\n",
    "obs_dist = mixture_rvs(prob=[weight1, weight2], size=250, \n",
    "                        dist=[stats.norm, stats.norm],\n",
    "                        kwargs = (dict(loc=dist1_loc, scale=dist1_scale),\n",
    "                                  dict(loc=dist2_loc, scale=dist2_scale)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The simplest non-parametric technique for density estimation is the histogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(obs_dist, np.abs(np.random.randn(obs_dist.size)), \n",
    "            zorder=15, color='red', marker='x', alpha=0.5, label='Samples')\n",
    "lines = ax.hist(obs_dist, bins=20, edgecolor='k', label='Histogram')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, zorder=-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit a continuous probability density function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = sm.nonparametric.KDEUnivariate(obs_dist)\n",
    "kde.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plotting the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the histrogram\n",
    "ax.hist(obs_dist, bins=20, normed=True, label='Histogram from samples', \n",
    "        zorder=5, edgecolor='k', alpha=0.5)\n",
    "\n",
    "# Plot the KDE as fitted using the default arguments\n",
    "ax.plot(kde.support, kde.density, lw=3, label='KDE from samples', zorder=10)\n",
    "\n",
    "# Plot the true distribution\n",
    "true_values = (stats.norm.pdf(loc=dist1_loc, scale=dist1_scale, x=kde.support)*weight1\n",
    "              + stats.norm.pdf(loc=dist2_loc, scale=dist2_scale, x=kde.support)*weight2)\n",
    "ax.plot(kde.support, true_values, lw=3, label='True distribution', zorder=15)\n",
    "\n",
    "# Plot the samples\n",
    "ax.scatter(obs_dist, np.abs(np.random.randn(obs_dist.size))/40, \n",
    "           marker='x', color='red', zorder=20, label='Samples', alpha=0.5)\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, zorder=-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `scikit-learn` is the most widely used ML library in Python\n",
    "-   standard supervised and unsupervised machine learning methods\n",
    "-   models that can be used for classification, clustering, prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/sklearn.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's download some datasets!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [https://www.sciencebase.gov/catalog/item/5a58af4fe4b00b291cd6a5fb>](https://www.sciencebase.gov/catalog/item/5a58af4fe4b00b291cd6a5fb>)and download these files\n",
    "\n",
    "-   `BASIN_CHARACTERISTICS.csv`\n",
    "-   `MON_P_CRU_19012015.csv`\n",
    "-   `MON_T_CRU_19012015.csv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "and load them up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "b = pd.read_csv(\"examples/BASIN_CHARACTERISTICS.csv\")\n",
    "p = pd.read_csv(\"examples/MON_P_CRU_19012015.csv\")\n",
    "t = pd.read_csv(\"examples/MON_T_CRU_19012015.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   One of the simplest unsupervised classification methods\n",
    "-   We will focus on the **k-means** clustering algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters data by trying to separate samples in n groups of equal variance\n",
    "\n",
    "$$\\sum_{i=0}^{n} \\min\\limits_{\\mu_{j} \\in C} (||x_{i} - \\mu_{j}||^{2})$$\n",
    "\n",
    "Inertia, or the within-cluster sum of squares criterion is a measure of how internally coherent clusters are. Let's look at an example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "plt.figure(figsize=(12, 12))\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's cluster the data and predict their label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "model = KMeans(n_clusters=3, random_state=random_state).fit(X)\n",
    "y_pred = model.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's try to classify the basins according to mean temperature and precipitation!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Create the appropriate arrays to be used as inputs to clustering algorithm\n",
    "-   Cluster basins according to precipitation\n",
    "-   Cluster basins according to temperature and precipitation\n",
    "-   Do you need to add any more features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we select the number of clusters?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the true labels are unknown, we can use Silhouette Analysis to evaluate the number of clusters\n",
    "\n",
    "Silhouette coefficient is defined as\n",
    "$$s = \\dfrac{b-a}{max(a,b)}$$\n",
    "where $a$ is the mean distance between a sample and all other points in the same class, and $b$ is the mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Silhouette analysis example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels = model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use Silhouette Analysis to evaluate the number of clusters for the basins\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Perform the clustering on the basins data and calculate the Silhouette coefficient as a function of number of clusters\n",
    "-   Plot the relationship between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The advantages of support vector machines are:\n",
    "\n",
    "-   Effective in high dimensional spaces\n",
    "-   Still effective in cases where number of dimensions is greater than the number of samples\n",
    "-   Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient\n",
    "-   Versatile: different Kernel functions can be specified for the decision function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We will focus on regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = svm.SVR()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now let's try to use SVMs to predict mean temperature as a function of elevation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Build a regression using SVMs on the basin data\n",
    "-   Split the dataset into training and validation subsets\n",
    "-   Is the regression model any good?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Homework\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](https://media.makeameme.org/created/no-homework-brwfr0.jpg)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
